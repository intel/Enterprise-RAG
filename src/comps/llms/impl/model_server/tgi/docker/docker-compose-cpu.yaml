# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

version: "3.8"

services:
  llm-tgi-model-server:
    image: ghcr.io/huggingface/text-generation-inference:2.1.0
    container_name: llm-tgi-model-server
    ports:
      - "${LLM_TGI_PORT}:80"
    volumes:
      - ./data:/data
    environment:
      - NO_PROXY=${NO_PROXY}
      - HTTP_PROXY=${HTTP_PROXY}
      - HTTPS_PROXY=${HTTPS_PROXY}
    command: ["--model-id", "${LLM_TGI_MODEL_NAME}", "--max-input-tokens", "256", "--max-total-tokens", "1024"]
    runtime: runc
    ipc: host

  llm_usvc:
    build:
      context: ../../../../../../
      dockerfile: ./comps/llms/impl/microservice/Dockerfile

    container_name: llm-tgi-microservice
    ipc: host
    runtime: runc
    network_mode: host
    environment:
      - NO_PROXY=${NO_PROXY}
      - HTTP_PROXY=${HTTP_PROXY}
      - HTTPS_PROXY=${HTTPS_PROXY}
      - LLM_MODEL_SERVER_ENDPOINT=http://localhost:${LLM_TGI_PORT}
      - LLM_MODEL_NAME=${LLM_MODEL_NAME}
      # Pass the environment variable if it is provided in .env file;
      # otherwise, use the defaults specified in reranks/config.ini
      - LLM_SERVICE_LOG_LEVEL:= ${LLM_SERVICE_LOG_LEVEL:-}
      - LLM_SERVICE_LOG_PATH:= ${LLM_SERVICE_LOG_PATH:-}
    restart: unless-stopped
    depends_on:
      - llm-tgi-model-server
