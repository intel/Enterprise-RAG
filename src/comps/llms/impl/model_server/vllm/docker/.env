## TGI Model Server Settings ##
LLM_VLLM_PORT=8009
LLM_VLLM_MODEL_NAME="Intel/neural-chat-7b-v3-3"
HUGGINGFACE_API_KEY=<your-hf-api-key>

# TODO: add devide specific parameters
#       like number of HPU devices, etc.

## Proxy settings ##
NO_PROXY=<your-no-proxy>
HTTP_PROXY=<your-http-proxy>
HTTPS_PROXY=<your-https-proxy>

## LLM Microservice Settings ##
# Logging
LOG_LEVEL=INFO
LOG_PATH=/tmp/opea/microservices/llm/llm.log

# OPEA Microservice
USVC_NAME=opea_service@llm
USVC_HOST=0.0.0.0
USVC_PORT=9000
