# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

version: "3.8"
# TODO: fix this
services:
  vllm-habana-image:
    image: vault.habana.ai/gaudi-docker/1.16.0/ubuntu22.04/habanalabs/pytorch-installer-2.2.2:latest
    environment:
      - LANG=en_US.UTF-8
      - no_proxy=localhost,127.0.0.1
      - PT_HPU_LAZY_ACC_PAR_MODE=0
      - PT_HPU_ENABLE_LAZY_COLLECTIVES=true
    working_dir: /root
    command: /bin/bash
    volumes:
      - .:/root
    ports:
      - "22:22"
    deploy:
      resources:
        limits:
          cpus: '0.50'
          memory: 512M
    restart: always
    build:
      context: .
      dockerfile: Dockerfile.hpu

  llm-vllm-model-server:
    image: vllm-habana-image
    container_name: vllm-gaudi-model-server
    ports:
      - "${LLM_VLLM_PORT}:80"
    volumes:
      - "./data:/data"
    environment:
      - NO_PROXY=${NO_PROXY}
      - HTTP_PROXY=${HTTP_PROXY}
      - HTTPS_PROXY=${HTTPS_PROXY}
      - HF_TOKEN=${HUGGINGFACEHUB_API_TOKEN}
      - HABANA_VISIBLE_DEVICES=all
      - OMPI_MCA_btl_vader_single_copy_mechanism=none
      - LLM_VLLM_MODEL_NAME=${LLM_VLLM_MODEL_NAME}
    runtime: habana
    cap_add:
      - SYS_NICE
    ipc: host
    command: /bin/bash -c "export VLLM_CPU_KVCACHE_SPACE=40 && python3 -m vllm.entrypoints.openai.api_server \
          --enforce-eager --model $LLM_VLLM_MODEL_NAME --tensor-parallel-size 1 \
          --host 0.0.0.0 --port 80"
    depends_on:
      - vllm-habana-image

  llm_usvc:
    build:
      context: ../../../../../../
      dockerfile: ./comps/llms/impl/microservice/Dockerfile

    container_name: llm-tgi-microservice
    ipc: host
    runtime: runc
    network_mode: host
    environment:
      - NO_PROXY=${NO_PROXY}
      - HTTP_PROXY=${HTTP_PROXY}
      - HTTPS_PROXY=${HTTPS_PROXY}
      - LLM_MODEL_SERVER_ENDPOINT=http://localhost:${LLM_VLLM_PORT}
      - LLM_MODEL_NAME=${LLM_VLLM_MODEL_NAME}
      # Pass the environment variable if it is provided in .env file;
      # otherwise, use the defaults specified in reranks/config.ini
      - USVC_NAME:= ${USVC_NAME:-}
      - LLM_SERVICE_LOG_PATH:= ${LLM_SERVICE_LOG_PATH:-}
    restart: unless-stopped
    depends_on:
      - llm-vllm-model-server
