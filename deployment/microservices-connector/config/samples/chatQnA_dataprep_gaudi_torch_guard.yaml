# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

apiVersion: gmc.opea.io/v1alpha3
kind: GMConnector
metadata:
  labels:
    app.kubernetes.io/name: gmconnector
    app.kubernetes.io/managed-by: kustomize
    gmc/platform: gaudi
  name: chatqa
  namespace: chatqa
spec:
  routerConfig:
    name: router
    serviceName: router-service
  nodes:
    root:
      routerType: Sequence
      steps:
      - name: Embedding
        internalService:
          serviceName: embedding-svc
          config:
            endpoint: /v1/embeddings
            EMBEDDING_MODEL_SERVER_ENDPOINT: torchserve-embedding-svc
            EMBEDDING_MODEL_NAME: "BAAI/bge-base-en-v1.5"
            EMBEDDING_MODEL_SERVER: "torchserve"
            FRAMEWORK: "langchain"
      - name: TorchserveEmbedding
        internalService:
          serviceName: torchserve-embedding-svc
          config:
            MODEL_NAME: "BAAI/bge-base-en-v1.5"
          isDownstreamService: true
      - name: Retriever
        data: $response
        internalService:
          serviceName: retriever-svc
          config:
            endpoint: /v1/retrieval
            REDIS_URL: redis-vector-db
            EMBED_MODEL: "BAAI/bge-base-en-v1.5"
      - name: VectorDB
        internalService:
          serviceName: redis-vector-db
          isDownstreamService: true
      - name: Reranking
        data: $response
        internalService:
          serviceName: reranking-svc
          config:
            endpoint: /v1/reranking
            TEI_RERANKING_ENDPOINT: tei-reranking-svc
      - name: TeiReranking
        internalService:
          serviceName: tei-reranking-svc
          config:
            endpoint: /rerank
          isDownstreamService: true
      - name: LLMGuardInput
        data: $response
        internalService:
          serviceName: input-scan-svc
          config:
            endpoint: /v1/llmguardinput
      - name: Llm
        data: $response
        internalService:
          serviceName: llm-svc
          config:
            endpoint: /v1/chat/completions
            TGI_LLM_ENDPOINT: tgi-gaudi-svc
            LLM_MODEL_SERVER_ENDPOINT: tgi-gaudi-svc
      - name: TgiGaudi
        internalService:
          serviceName: tgi-gaudi-svc
          config:
            endpoint: /generate
          isDownstreamService: true
      - name: DataPrep
        internalService:
          serviceName: data-prep-svc
          config:
            endpoint: /v1/dataprep
            REDIS_URL: redis-vector-db
            TEI_ENDPOINT: torchserve-embedding-svc
          isDownstreamService: true
